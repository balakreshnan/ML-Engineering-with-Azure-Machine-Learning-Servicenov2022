{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps with CLI V2\n",
    "\n",
    "- Creating Scripts for MLOps Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mlflow azureml-mlflow\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?tabs=interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aml-workspace'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription_id = os.environ.get('subscription_id')\n",
    "resource_group = os.environ.get('resource_group')\n",
    "workspace = os.environ.get('workspace')\n",
    "\n",
    "os.environ.setdefault('subscription_id', '5da07161-3770-4a4b-aa43-418cbbb627cf')\n",
    "os.environ.setdefault('resource_group', 'aml-workspace-rg')\n",
    "os.environ.setdefault('workspace', 'aml-workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import Environment, BuildContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '5da07161-3770-4a4b-aa43-418cbbb627cf'\n",
    "resource_group = 'aml-workspace-rg'\n",
    "workspace = 'aml-workspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/prep prep folder created\n",
      "./src/train train folder created\n",
      "./src/eval eval folder created\n",
      "./src/deploy deploy folder created\n",
      "./src/pipeline pipeline folder created\n",
      "./src/conda-yamls conda-yamls folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "script_folder = './src/prep'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'prep folder created')\n",
    "\n",
    "script_folder = './src/train'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'train folder created')\n",
    "\n",
    "script_folder = './src/eval'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'eval folder created')\n",
    "\n",
    "script_folder = './src/deploy'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'deploy folder created')\n",
    "\n",
    "script_folder = './src/pipeline'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'pipeline folder created')\n",
    "\n",
    "script_folder = './src/conda-yamls'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'conda-yamls folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/conda-yamls/pipeline_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/conda-yamls/pipeline_env.yml\n",
    "name: job_env\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.8.5\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow\n",
    "  - azureml-mlflow==1.43.0.post1\n",
    "  - azure-ai-ml\n",
    "  - mltable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    # This will open a browser page for\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanzsymtest/code/Users/memasanz/.azureml/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7f8d5eab87c0>,\n",
      "         subscription_id=5da07161-3770-4a4b-aa43-418cbbb627cf,\n",
      "         resource_group_name=aml-workspace-rg,\n",
      "         workspace_name=aml-workspace)\n"
     ]
    }
   ],
   "source": [
    "#connect to the workspace\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": workspace,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileInfo: file_size=-1, is_dir=True, path='logs'>,\n",
       " <FileInfo: file_size=-1, is_dir=True, path='system_logs'>,\n",
       " <FileInfo: file_size=-1, is_dir=True, path='user_logs'>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.list_artifacts(\"000d6ec3-533a-4ed7-8644-667da5d7a8d8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "data asset is registered\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('./data/titanic.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "try:\n",
    "    registered_data_asset = ml_client.data.get(name='titanic_raw', version=1)\n",
    "    print('data asset is registered')\n",
    "except:\n",
    "    print('register data asset')\n",
    "    my_data = Data(\n",
    "        path=\"./data/titanic.csv\",\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Titanic CSV\",\n",
    "        name=\"titanic_raw\",\n",
    "        version=\"1\",\n",
    "    )\n",
    "\n",
    "    ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Scripts for Training Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prep/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep/prep.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--raw_data\", type=str, help=\"Path to raw data\")\n",
    "parser.add_argument(\"--prep_data\", type=str, help=\"Path of prepped data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.raw_data)\n",
    "print(args.prep_data)\n",
    "\n",
    "#/mnt/azureml/cr/j/f91687126d3744229274c6ecbe85b820/cap/data-capability/wd/INPUT_raw_data\n",
    "\n",
    "df = pd.read_csv(args.raw_data + '/titanic.csv')\n",
    "\n",
    "df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "df['Sex']= df['Sex'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df['Loc']= df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "df.loc[:,'GroupSize'] = 1 + df['SibSp'] + df['Parch']\n",
    "\n",
    "LABEL = 'Survived'\n",
    "df_train = df\n",
    "df = df_train.drop(['Name','SibSp', 'Parch', 'PassengerId'], axis=1)\n",
    "\n",
    "df.to_csv(args.prep_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train/train.py\n",
    "import os\n",
    "import mlflow\n",
    "import argparse\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# define functions\n",
    "def main(args):\n",
    "    # enable auto logging\n",
    "    current_run = mlflow.start_run()\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # read in data\n",
    "    print('about to read file:' + args.prep_data)\n",
    "    df = pd.read_csv(args.prep_data)\n",
    "    #model, X_test = model_train('Survived', df, args.randomstate)\n",
    "    model, X_test, y_test = model_train('Survived', df, 0)\n",
    "    \n",
    "    model_file = os.path.join(args.model_output, 'titanic_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    \n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    y_test.to_csv('outputs/Y_test.csv', index = False)\n",
    "    X_test.to_csv( 'outputs/X_test.csv', index = False)\n",
    "    shutil.copytree('./outputs/', args.test_data, dirs_exist_ok=True)\n",
    "    #maybe do mlflow logmodel\n",
    "    mlflow.sklearn.log_model(model, \"championmodel\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def model_train(LABEL, df, randomstate):\n",
    "    print('df.columns = ')\n",
    "    print(df.columns)\n",
    "    y_raw           = df[LABEL]\n",
    "    columns_to_keep = ['Embarked', 'Loc', 'Sex','Pclass', 'Age', 'Fare', 'GroupSize']\n",
    "    X_raw           = df[columns_to_keep]\n",
    "    \n",
    "    X_raw['Embarked'] = X_raw['Embarked'].astype(object)\n",
    "    X_raw['Loc'] = X_raw['Loc'].astype(object)\n",
    "    X_raw['Loc'] = X_raw['Sex'].astype(object)\n",
    "    X_raw['Pclass'] = X_raw['Pclass'].astype(float)\n",
    "    X_raw['Age'] = X_raw['Age'].astype(float)\n",
    "    X_raw['Fare'] = X_raw['Fare'].astype(float)\n",
    "    X_raw['GroupSize'] = X_raw['GroupSize'].astype(float)\n",
    "    \n",
    "\n",
    "\n",
    "    print(X_raw.columns)\n",
    "     # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=args.randomstate)\n",
    "    \n",
    "    #use Logistic Regression estimator from scikit learn\n",
    "    lg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "    preprocessor = buildpreprocessorpipeline(X_train)\n",
    "    \n",
    "    #estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lg)], verbose=True)\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('type of X_test = ' + str(type(X_test)))\n",
    "          \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print('*****X_test************')\n",
    "    print(X_test)\n",
    "    \n",
    "    metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=\"test_\")\n",
    "    \n",
    "    #get the active run.\n",
    "    run = mlflow.active_run()\n",
    "    print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "    MlflowClient().log_metric(run.info.run_id, \"metric\", 0.22)\n",
    "\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "    #mlflow.end_run()\n",
    "\n",
    "\n",
    "def buildpreprocessorpipeline(X_raw):\n",
    "\n",
    "    categorical_features = X_raw.select_dtypes(include=['object', 'bool']).columns\n",
    "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('onehotencoder', \n",
    "                                               OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "    numeric_transformer1 = Pipeline(steps=[('scaler1', SimpleImputer(missing_values=np.nan, strategy = 'mean'))])\n",
    "    \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric1', numeric_transformer1, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)], remainder='drop')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--prep_data\", default=\"data\", type=str, help=\"Path to prepped data, default to local folder\")\n",
    "    parser.add_argument(\"--input_file_name\", type=str, default=\"titanic.csv\")\n",
    "    parser.add_argument(\"---randomstate\", type=int, default=42)\n",
    "#     \n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "    parser.add_argument(\"--test_data\", type=str,)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    print(args.prep_data)\n",
    "    print(args.input_file_name)\n",
    "    print(args.randomstate)\n",
    "    print(args.model_output)\n",
    "    print(args.test_data)\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/eval/eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/eval/eval.py\n",
    "import os\n",
    "import mlflow\n",
    "import argparse\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from azureml.core import Run\n",
    "from azureml.core import Model\n",
    "\n",
    "def create_deploymentfiles(endpoint_name, model_name):\n",
    "    outputfolder = \"outputs\"\n",
    "    os.makedirs(outputfolder, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(outputfolder, 'create-endpoint.yaml'), \"w+\") as f:\n",
    "        f.write('$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json \\n')\n",
    "        f.write('name: ' + endpoint_name + '\\n')\n",
    "        f.write('auth_mode: key \\n')\n",
    "        \n",
    "    with open(os.path.join(outputfolder, 'model_deployment.yaml'), \"w+\") as f:\n",
    "        f.write('$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json \\n')\n",
    "        f.write('name: ' + endpoint_name + '\\n')\n",
    "        f.write('model: azureml:' + model_name + '@latest \\n')\n",
    "        f.write('instance_type: Standard_DS2_v2 \\n')\n",
    "        f.write('instance_count: 1  \\n')\n",
    "\n",
    "\n",
    "    #shutils.copy model_deployment_files    \n",
    "    shutil.copytree('./outputs/', args.model_deployment_files, dirs_exist_ok=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# define functions\n",
    "def main(args):\n",
    "    \n",
    "    model_name = args.model_name\n",
    "        \n",
    "    # read in data\n",
    "    print('about to read file:' + args.test_data)\n",
    "    X_test = pd.read_csv(args.test_data + '/X_test.csv')\n",
    "    df_y_test = pd.read_csv(args.test_data + '/Y_test.csv')\n",
    "    y_test  = df_y_test.values.flatten()\n",
    "    #load champion model\n",
    "    model_file = os.path.join(args.model_folder, 'titanic_model.pkl')\n",
    "    champion_model = joblib.load(model_file)\n",
    "    \n",
    "    y_pred_current = champion_model.predict(X_test)\n",
    "    print('y_pred_current')\n",
    "    print(y_pred_current)\n",
    "    print('y_test')\n",
    "    print(y_test)\n",
    "    \n",
    "    champion_auc = roc_auc_score(y_test,y_pred_current)\n",
    "    print('champion_auc:' , champion_auc)\n",
    "    \n",
    "    champion_acc = np.average(y_pred_current == y_test)\n",
    "    print('champion_acc:', champion_acc)\n",
    "\n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "    run_id = run.id\n",
    "    print('run_id =' + run_id)\n",
    "    model_list = Model.list(ws, name=model_name, latest=True)\n",
    "    first_registration = len(model_list)==0\n",
    "    current_model = None\n",
    "    \n",
    "    try:\n",
    "        current_model_aml = Model(ws,args.model_name)\n",
    "        os.makedirs(\"current_model\", exist_ok=True)\n",
    "        current_model_aml.download(\"current_model\",exist_ok=True)\n",
    "        current_model = mlflow.sklearn.load_model(os.path.join(\"current_model\",args.model_name))\n",
    "    except:\n",
    "        print('no model register with name' + args.model_name)\n",
    "        pass\n",
    "    \n",
    "    if current_model:\n",
    "        y_pred_current = current_model.predict(X_test)\n",
    "        current_acc = np.average(y_pred_current == y_test)\n",
    "        if champion_acc >= current_acc:\n",
    "            print('better model found, registering')\n",
    "            mlflow.sklearn.log_model(champion_model,args.model_name)\n",
    "            model_uri = f'runs:/{run_id}/{args.model_name}'\n",
    "            mlflow.register_model(model_uri,args.model_name)\n",
    "            create_deploymentfiles('Chapter8titanicendpoint', model_name)\n",
    "            \n",
    "        else:\n",
    "            print('current model performs better than champion model')\n",
    "    else:\n",
    "        print('no current model')\n",
    "        print(\"First time model train, registering\")\n",
    "        mlflow.sklearn.log_model(champion_model,args.model_name)\n",
    "        model_uri = f'runs:/{run_id}/{args.model_name}'\n",
    "        mlflow.register_model(model_uri,args.model_name)\n",
    "        create_deploymentfiles('Chapter8titanicendpoint', model_name)\n",
    "        print('hello')\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--test_data\", default=\"data\", type=str, help=\"Path to test data\")\n",
    "    parser.add_argument(\"--model_folder\", default=\"data\", type=str, help=\"Path to model data\")\n",
    "    parser.add_argument(\"--model_name\",default='mmchapter8titanic',type=str, help=\"Name of the model in workspace\")\n",
    "\n",
    "    parser.add_argument(\"--model_deployment_files\", default=\"data\", type=str, help=\"Path to model data\")\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(args.test_data)\n",
    "    print(args.model_folder)\n",
    "    print(args.model_name)\n",
    "        \n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipeline/train_and_eval_pipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipeline/train_and_eval_pipeline.yml\n",
    "\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "display_name: Training_and_eval_pipeline\n",
    "experiment_name: Training_and_eval_pipeline\n",
    "compute: azureml:cpu-cluster\n",
    "\n",
    "jobs:\n",
    "  prep_job:\n",
    "    type: command\n",
    "    code: ../prep\n",
    "    command: >-\n",
    "      python prep.py \n",
    "      --raw_data ${{inputs.raw_data}}\n",
    "      --prep_data ${{outputs.prep_data}}\n",
    "    inputs:\n",
    "      raw_data:\n",
    "        type: uri_folder\n",
    "        path: azureml:titanic_raw:1\n",
    "        mode: ro_mount\n",
    "    outputs:\n",
    "      prep_data:\n",
    "        type: uri_file\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_prep_data/titanic_prepped.csv\n",
    "        mode: rw_mount\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    description: Feature Engineering\n",
    "    \n",
    "  train_job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      prep_data: ${{parent.jobs.prep_job.outputs.prep_data}}\n",
    "    outputs:\n",
    "      model_output:\n",
    "        type: uri_folder\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_model_data/\n",
    "        mode: rw_mount\n",
    "      test_data: \n",
    "        type: uri_folder\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_test_data/\n",
    "        mode: rw_mount\n",
    "    code: ../train\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --prep_data ${{inputs.prep_data}} \n",
    "      --model_output ${{outputs.model_output}}\n",
    "      --test_data ${{outputs.test_data}}\n",
    "\n",
    "  eval_job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      test_data: ${{parent.jobs.train_job.outputs.test_data}}\n",
    "      model_folder: ${{parent.jobs.train_job.outputs.model_output}}\n",
    "    outputs:\n",
    "      model_deployment_files:\n",
    "        type: uri_folder\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_model_deployment_files/\n",
    "        mode: rw_mount\n",
    "    code: ../eval\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/pipeline_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python eval.py \n",
    "      --test_data ${{inputs.test_data}} \n",
    "      --model_folder ${{inputs.model_folder}}\n",
    "      --model_deployment_files $${{outputs.model_deployment_files}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/pipeline/AzureDevOpsPipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipeline/AzureDevOpsPipeline.yml\n",
    "\n",
    "resources:\n",
    "  containers:\n",
    "  - container: mlops\n",
    "    image: mcr.microsoft.com/mlops/python:latest\n",
    "\n",
    "pr: none\n",
    "trigger:\n",
    "  branches:\n",
    "    include:\n",
    "    - main\n",
    "\n",
    "variables:\n",
    "- group: xmmdevops-variable-group-non-prod\n",
    "- group: xmmdevops-variable-group-qa\n",
    "- name: initalmodelversion\n",
    "  value: initialValue\n",
    "- name: finalmodelversion\n",
    "  value: initialValue\n",
    "\n",
    "\n",
    "pool:\n",
    "  vmImage: ubuntu-latest\n",
    "\n",
    "stages:\n",
    "- stage: 'RunPipline'\n",
    "  variables:\n",
    "  - group: xmmdevops-variable-group-non-prod\n",
    "  displayName: 'TrainingPipeline'\n",
    "  jobs:\n",
    "  - job: \"TrainingPipeline\"\n",
    "    steps:\n",
    "      - task: UsePythonVersion@0\n",
    "        inputs:\n",
    "          versionSpec: '3.8'\n",
    "          addToPath: true\n",
    "      - script: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install jupyter\n",
    "          pip install nbconvert\n",
    "          pip install --upgrade azureml-core\n",
    "          pip install --upgrade azureml-sdk[automl]\n",
    "      \n",
    "      - task: Bash@3\n",
    "        inputs:\n",
    "          workingDirectory: $(Build.SourcesDirectory)\n",
    "          targetType: 'inline'\n",
    "          script: |\n",
    "            echo \"##vso[task.setvariable variable=initalmodelversion;isOutput=true]$(az ml model list -w aml-workspace -g aml-workspace-rg  -n mmchapter8titanic --query \"[0].version\" -o tsv)\"\n",
    "            echo $(initalmodelversion)\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          tenantId: $(tenantId)\n",
    "          servicePrincipalId: $(servicePrincipalId)\n",
    "          servicePrincipalPassword: $(servicePrincipalPassword)\n",
    "          wsName: $(wsName)\n",
    "          subscriptionId: $(subscriptionId)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: dev-aml-workspace-connection\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"files:\"\n",
    "            ls\n",
    "            az version\n",
    "            az extension add -n ml -y\n",
    "            az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "            az ml job create --file Chapter8/src/pipeline/train_and_eval_pipeline.yml --stream\n",
    "        displayName: 'Training Pipeline'\n",
    "            \n",
    "      - task: Bash@3\n",
    "        inputs:\n",
    "          workingDirectory: $(Build.SourcesDirectory)\n",
    "          targetType: 'inline'\n",
    "          script: |\n",
    "            echo \"##vso[task.setvariable variable=finalmodelversion;isOutput=true]$(az ml model list -w aml-workspace -g aml-workspace-rg  -n mmchapter8titanic --query \"[0].version\" -o tsv)\"\n",
    "            echo $(finalmodelversion)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create eval.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "# import datetime\n",
    "\n",
    "# online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# # create an online endpoint\n",
    "# endpoint = ManagedOnlineEndpoint(\n",
    "#     name=online_endpoint_name,\n",
    "#     description=\"titanic online endpoint for mlflow model\",\n",
    "#     auth_mode=\"key\",\n",
    "#     tags={\"oneline endpoint\": \"titanic\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/endpoint.yml\n",
    "# $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "# name: titanic-managed-online-endpoint\n",
    "# description: \"CLI V2 titanic online endpoint for mlflow model\"\n",
    "# auth_mode: key\n",
    "# tags : {\"CLIV2\": \"titanic\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/deployment.yml\n",
    "\n",
    "# $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "# name: blue\n",
    "# endpoint_name: titanic-managed-online-endpoint\n",
    "# model: azureml:chapter6_titanic_model:1\n",
    "# code_configuration:\n",
    "#   code: \n",
    "#     local_path: .\n",
    "#   scoring_script: score.py\n",
    "# environment: azureml:job_base_env:1\n",
    "# instance_type: Standard_F2s_v2\n",
    "# instance_count: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/score.py\n",
    "\n",
    "# import os \n",
    "# import json\n",
    "# import joblib\n",
    "# from pandas import json_normalize\n",
    "# import pandas as pd\n",
    "\n",
    "# # Called when the service is loaded\n",
    "# def init():\n",
    "#     global model\n",
    "#     # Get the path to the deployed model file and load it\n",
    "#     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'titanic_model.pkl')\n",
    "#     model = joblib.load(model_path)\n",
    "\n",
    "# # Called when a request is received\n",
    "# def run(raw_data):\n",
    "#     dict= json.loads(raw_data)\n",
    "#     df = json_normalize(dict['raw_data']) \n",
    "#     y_pred = model.predict(df)\n",
    "#     print(type(y_pred))\n",
    "    \n",
    "#     result = {\"result\": y_pred.tolist()}\n",
    "#     return result"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
