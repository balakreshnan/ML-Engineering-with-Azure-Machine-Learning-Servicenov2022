{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps with CLI V2\n",
    "\n",
    "- Creating Scripts for MLOps Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import Environment, BuildContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '5da07161-3770-4a4b-aa43-418cbbb627cf'\n",
    "resource_group = 'aml-workspace-rg'\n",
    "workspace = 'aml-workspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/prep prep folder created\n",
      "./src/train train folder created\n",
      "./src/deploy deploy folder created\n",
      "./src/pipeline pipeline folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "script_folder = './src/prep'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'prep folder created')\n",
    "\n",
    "script_folder = './src/train'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'train folder created')\n",
    "\n",
    "script_folder = './src/deploy'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'deploy folder created')\n",
    "\n",
    "script_folder = './src/pipeline'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'pipeline folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanzsymtest/code/Users/memasanz/ML-Engineering-with-Azure-Machine-Learning-Service/Chapter8/conda-yamls\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"conda-yamls\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanzsymtest/code/Users/memasanz/ML-Engineering-with-Azure-Machine-Learning-Service/Chapter8/conda-yamls/job_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/job_env.yml\n",
    "name: job_env\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.8.5\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow\n",
    "  - azureml-mlflow==1.43.0.post1\n",
    "  - azure-ai-ml\n",
    "  - mltable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    # This will open a browser page for\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanzsymtest/code/Users/memasanz/.azureml/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7ff444f7f6a0>,\n",
      "         subscription_id=5da07161-3770-4a4b-aa43-418cbbb627cf,\n",
      "         resource_group_name=aml-workspace-rg,\n",
      "         workspace_name=aml-workspace)\n"
     ]
    }
   ],
   "source": [
    "#connect to the workspace\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": workspace,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "data asset is registered\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('./data/titanic.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "try:\n",
    "    registered_data_asset = ml_client.data.get(name='titanic_raw', version=1)\n",
    "    print('data asset is registered')\n",
    "except:\n",
    "    print('register data asset')\n",
    "    my_data = Data(\n",
    "        path=\"./data/titanic.csv\",\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        description=\"Titanic CSV\",\n",
    "        name=\"titanic_raw\",\n",
    "        version=\"1\",\n",
    "    )\n",
    "\n",
    "    ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Scripts for Training Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/prep/prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prep/prep.py\n",
    "\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser(\"prep\")\n",
    "parser.add_argument(\"--raw_data\", type=str, help=\"Path to raw data\")\n",
    "parser.add_argument(\"--prep_data\", type=str, help=\"Path of prepped data\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.raw_data)\n",
    "print(args.prep_data)\n",
    "\n",
    "\n",
    "df = pd.read_csv(args.raw_data)\n",
    "\n",
    "df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "df['Sex']= df['Sex'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df['Loc']= df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n",
    "df.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\n",
    "df['Embarked'] = df['Embarked'].fillna('S')\n",
    "df.loc[:,'GroupSize'] = 1 + df['SibSp'] + df['Parch']\n",
    "\n",
    "LABEL = 'Survived'\n",
    "columns_to_keep = ['Pclass', 'Sex','Age', 'Fare', 'Embared', 'Deck', 'GroupSize']\n",
    "columns_to_drop = ['Name','SibSp', 'Parch', 'Survived']\n",
    "df_train = df\n",
    "df = df_train.drop(['Name','SibSp', 'Parch', 'PassengerId'], axis=1)\n",
    "\n",
    "df.to_csv(args.prep_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/train/train.py\n",
    "import os\n",
    "import mlflow\n",
    "import argparse\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# define functions\n",
    "def main(args):\n",
    "    # enable auto logging\n",
    "    current_run = mlflow.start_run()\n",
    "    #mlflow.sklearn.autolog()\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_csv(args.titanic_csv)\n",
    "    model, X_test = model_train('Survived', df, args.randomstate)\n",
    "    \n",
    "    model_file = os.path.join('outputs', 'titanic_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    \n",
    "    shutil.copy('./outputs/titanic_model.pkl', os.path.join(args.model_output, \"titanic_model.pkl\"))\n",
    "    \n",
    "    \n",
    "    X_test.to_csv(args.test_data)\n",
    "\n",
    "def model_train(LABEL, df, randomstate):\n",
    "    print('df.columns = ')\n",
    "    print(df.columns)\n",
    "    y_raw           = df[LABEL]\n",
    "    columns_to_keep = ['Embarked', 'Loc', 'Sex','Pclass', 'Age', 'Fare', 'GroupSize']\n",
    "    X_raw           = df[columns_to_keep]\n",
    "    \n",
    "    X_raw['Embarked'] = X_raw['Embarked'].astype(object)\n",
    "    X_raw['Loc'] = X_raw['Loc'].astype(object)\n",
    "    X_raw['Loc'] = X_raw['Sex'].astype(object)\n",
    "    X_raw['Pclass'] = X_raw['Pclass'].astype(float)\n",
    "    X_raw['Age'] = X_raw['Age'].astype(float)\n",
    "    X_raw['Fare'] = X_raw['Fare'].astype(float)\n",
    "    X_raw['GroupSize'] = X_raw['GroupSize'].astype(float)\n",
    "    \n",
    "\n",
    "\n",
    "    print(X_raw.columns)\n",
    "     # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=randomstate)\n",
    "    \n",
    "    #use Logistic Regression estimator from scikit learn\n",
    "    lg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "    preprocessor = buildpreprocessorpipeline(X_train)\n",
    "    \n",
    "    #estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lg)], verbose=True)\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('type of X_test = ' + str(type(X_test)))\n",
    "          \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print('*****X_test************')\n",
    "    print(X_test)\n",
    "    \n",
    "    metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=\"test_\")\n",
    "    \n",
    "    #get the active run.\n",
    "    run = mlflow.active_run()\n",
    "    print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "    MlflowClient().log_metric(run.info.run_id, \"metric\", 0.22)\n",
    "\n",
    "    \n",
    "    return model, X_test\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "def buildpreprocessorpipeline(X_raw):\n",
    "\n",
    "    categorical_features = X_raw.select_dtypes(include=['object', 'bool']).columns\n",
    "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('onehotencoder', \n",
    "                                               OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "    numeric_transformer1 = Pipeline(steps=[('scaler1', SimpleImputer(missing_values=np.nan, strategy = 'mean'))])\n",
    "    \n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric1', numeric_transformer1, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)], remainder='drop')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"---training_data\", type=str)\n",
    "    parser.add_argument(\"---randomstate\", type=int, default=42)\n",
    "    parser.add_argument(\"--test_data\", type=str,)\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/pipeline/train_and_eval_pipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipeline/train_and_eval_pipeline.yml\n",
    "\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json\n",
    "type: pipeline\n",
    "display_name: Training_and_eval_pipeline\n",
    "experiment_name: Training_and_eval_pipeline\n",
    "compute: azureml:cpu-cluster\n",
    "\n",
    "jobs:\n",
    "  prep_job:\n",
    "    type: command\n",
    "    code: ../prep\n",
    "    command: >-\n",
    "      python prep.py \n",
    "      --raw_data ${{inputs.raw_data}}\n",
    "      --prep_data ${{outputs.prep_data}}\n",
    "    inputs:\n",
    "      raw_data:\n",
    "        type: uri_folder\n",
    "        path: azureml:titanic_raw:1\n",
    "        mode: ro_mount\n",
    "    outputs:\n",
    "      prep_data:\n",
    "        type: uri_file\n",
    "        path: azureml://datastores/workspaceblobstore/paths/titanic_prep_data/titanic_prepped.csv\n",
    "        mode: rw_mount\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/job_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    description: Feature Engineering\n",
    "        \n",
    "  train_job:\n",
    "    type: command\n",
    "    inputs:\n",
    "      training_data: ${{parent.jobs.prep_job.outputs.prep_data}}\n",
    "      randomstate: 0\n",
    "    outputs:\n",
    "      model_output: \n",
    "      test_data: \n",
    "        mode: upload\n",
    "    code: ../train\n",
    "    environment:\n",
    "      conda_file: ../conda-yamls/job_env.yml\n",
    "      image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\n",
    "    compute: azureml:cpu-cluster\n",
    "    command: >-\n",
    "      python train.py \n",
    "      --training_data ${{inputs.training_data}} \n",
    "      --randomstate ${{inputs.randomstate}} \n",
    "      --test_data ${{outputs.test_data}} \n",
    "      --model_output ${{outputs.model_output}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/pipeline/AzureDevOpsPipeline.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pipeline/AzureDevOpsPipeline.yml\n",
    "\n",
    "resources:\n",
    "  containers:\n",
    "  - container: mlops\n",
    "    image: mcr.microsoft.com/mlops/python:latest\n",
    "\n",
    "pr: none\n",
    "trigger:\n",
    "  branches:\n",
    "    include:\n",
    "    - main\n",
    "\n",
    "variables:\n",
    "- group: xmmdevops-variable-group-non-prod\n",
    "- group: xmmdevops-variable-group-qa\n",
    "\n",
    "pool:\n",
    "  vmImage: ubuntu-latest\n",
    "\n",
    "stages:\n",
    "- stage: 'RunPipline'\n",
    "  variables:\n",
    "  - group: xmmdevops-variable-group-qa\n",
    "  displayName: 'Register Model QA'\n",
    "  jobs:\n",
    "  - job: \"RegisterQA\"\n",
    "    steps:\n",
    "      - task: UsePythonVersion@0\n",
    "        inputs:\n",
    "          versionSpec: '3.8'\n",
    "          addToPath: true\n",
    "      - script: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install jupyter\n",
    "          pip install nbconvert\n",
    "          pip install --upgrade azureml-core\n",
    "          pip install --upgrade azureml-sdk[automl]\n",
    "            \n",
    "      - task: AzureCLI@1\n",
    "        env:\n",
    "          tenantId: $(tenantId)\n",
    "          servicePrincipalId: $(servicePrincipalId)\n",
    "          servicePrincipalPassword: $(servicePrincipalPassword)\n",
    "          wsName: $(wsName)\n",
    "          subscriptionId: $(subscriptionId)\n",
    "          resourceGroup: $(resourceGroup)\n",
    "          location: $(location)\n",
    "        inputs:\n",
    "          azureSubscription: dev-aml-workspace-connection\n",
    "          scriptLocation: inlineScript\n",
    "          workingDirectory: '$(Build.SourcesDirectory)'\n",
    "          inlineScript: |\n",
    "            echo \"files:\"\n",
    "            ls\n",
    "            az version\n",
    "            az extension add -n ml -y\n",
    "            az configure --defaults group=$(resourceGroup) workspace=$(wsName) location=$(location)\n",
    "            az ml job create -s -f src/pipeline/AzureDevOpsPipeline.yml\n",
    "        displayName: 'Training Pipeline'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create eval.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "# import datetime\n",
    "\n",
    "# online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# # create an online endpoint\n",
    "# endpoint = ManagedOnlineEndpoint(\n",
    "#     name=online_endpoint_name,\n",
    "#     description=\"titanic online endpoint for mlflow model\",\n",
    "#     auth_mode=\"key\",\n",
    "#     tags={\"oneline endpoint\": \"titanic\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/endpoint.yml\n",
    "# $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "# name: titanic-managed-online-endpoint\n",
    "# description: \"CLI V2 titanic online endpoint for mlflow model\"\n",
    "# auth_mode: key\n",
    "# tags : {\"CLIV2\": \"titanic\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/deployment.yml\n",
    "\n",
    "# $schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "# name: blue\n",
    "# endpoint_name: titanic-managed-online-endpoint\n",
    "# model: azureml:chapter6_titanic_model:1\n",
    "# code_configuration:\n",
    "#   code: \n",
    "#     local_path: .\n",
    "#   scoring_script: score.py\n",
    "# environment: azureml:job_base_env:1\n",
    "# instance_type: Standard_F2s_v2\n",
    "# instance_count: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile $script_folder/score.py\n",
    "\n",
    "# import os \n",
    "# import json\n",
    "# import joblib\n",
    "# from pandas import json_normalize\n",
    "# import pandas as pd\n",
    "\n",
    "# # Called when the service is loaded\n",
    "# def init():\n",
    "#     global model\n",
    "#     # Get the path to the deployed model file and load it\n",
    "#     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'titanic_model.pkl')\n",
    "#     model = joblib.load(model_path)\n",
    "\n",
    "# # Called when a request is received\n",
    "# def run(raw_data):\n",
    "#     dict= json.loads(raw_data)\n",
    "#     df = json_normalize(dict['raw_data']) \n",
    "#     y_pred = model.predict(df)\n",
    "#     print(type(y_pred))\n",
    "    \n",
    "#     result = {\"result\": y_pred.tolist()}\n",
    "#     return result"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
