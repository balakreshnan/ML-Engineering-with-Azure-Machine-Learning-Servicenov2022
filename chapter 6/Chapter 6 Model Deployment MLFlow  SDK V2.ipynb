{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy MLflow Model to online endpoint\n",
    "\n",
    "- Before running this notebook, run the **Chapter 6 Prep-Model Creation & Registration.ipynb** notebook to create and register a model for use\n",
    "\n",
    "- Models created with MLFlow do not require a scoring script nor an environment\n",
    "\n",
    "## In this notebook we will:\n",
    "\n",
    "- Connect to your workspace.\n",
    "- Create an online endpoint\n",
    "- Retrieve and register a model from the job ran in the previous notebook\n",
    "- Create a deployment\n",
    "- Make an API Call to the managed online endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import azure.ai.ml\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "\n",
    "print(azure.ai.ml._version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
    "resource_group = \"<RESOURCE_GROUP>\"\n",
    "workspace = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    # This will open a browser page for\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz5/code/Users/memasanz/ML-Engineering-with-Azure-Machine-Learning-Service/.azureml/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7efffa1cbf70>,\n",
      "         subscription_id=5da07161-3770-4a4b-aa43-418cbbb627cf,\n",
      "         resource_group_name=mm-aml-mlops-dev-rg,\n",
      "         workspace_name=mm-aml-mlops-dev)\n"
     ]
    }
   ],
   "source": [
    "#connect to the workspace\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": workspace,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Online Endpoint\n",
    "\n",
    "To create an online endpoint, we will leverage the class *ManagedOnlineEndpoint*.  To create the online endpoint we will provide the following configuration:\n",
    "\n",
    "- name of endpoint\n",
    "- description\n",
    "- auth_mode (set to key) or aml_token\n",
    "- tags - to provide information regarding the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"chp6-mlflow-endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "print(len(online_endpoint_name))\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"titanic online endpoint for mlflow model\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"mlflow\": \"true\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create endpoint\n",
    "\n",
    "Using the MLClient created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f0007c5a560>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment\n",
    "A deployment is a set of resouces used for hosting the inferecing model using the *ManagedOnlineDeployment* class.  \n",
    "Using the *ManagedOnlineDeployment* class, a developer can configure the following components\n",
    "\n",
    "- name: name of the deployment\n",
    "- endpoint_name: name of the endpoint to create the deployment under\n",
    "- model: the model to use for the deployment\n",
    "- instance_type: the VM side to use for deployment\n",
    "- instance_count: the number of instances to use for the deployment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Model from registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1f51d714-6c5b-47ea-9212-873cc346f796\n",
      "busy_store_ly2m7jfdlw\n"
     ]
    }
   ],
   "source": [
    "experiment = 'chapter6'\n",
    "current_experiment=dict(mlflow.get_experiment_by_name(experiment))\n",
    "experiment_id=current_experiment['experiment_id']\n",
    "print(experiment_id)\n",
    "\n",
    "df = mlflow.search_runs([experiment_id])\n",
    "\n",
    "run_id = df['run_id'].tail(1).values[0]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "run_model = Model(\n",
    "    name=\"chapter6_titanic_model\",\n",
    "    path=\"azureml://jobs/\" + run_id + \"/outputs/artifacts/paths/model/\",\n",
    "    description=\"Model created from run.\",\n",
    "    type=AssetTypes.MLFLOW_MODEL \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'is_anonymous': False, 'auto_increment_version': True, 'name': 'chapter6_titanic_model', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz5/code/Users/memasanz/ML-Engineering-with-Azure-Machine-Learning-Service/chapter 6', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f0007c3c700>, 'version': None, 'latest_version': None, 'path': 'azureml://jobs/busy_store_ly2m7jfdlw/outputs/artifacts/paths/model/', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'mlflow_model'})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the deployment\n",
    "\n",
    "- retrieve the experiment id for this run, and the run id to retrieve the model from the registered model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while ml_client.online_endpoints.get(name=online_endpoint_name).provisioning_state == 'Creating':\n",
    "    print('Creating')\n",
    "    time.sleep(10)\n",
    "\n",
    "print(ml_client.online_endpoints.get(name=online_endpoint_name).provisioning_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=run_model,\n",
    "    instance_type=\"Standard_F4s_v2\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chp6-mlflow-endpt-11302038545236\n"
     ]
    }
   ],
   "source": [
    "## Create deployment\n",
    "print(online_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint chp6-mlflow-endpt-11302038545236 exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f000c1f26b0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "..Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      ".Updating..will take about 10 minutes to deploy...\n",
      "."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Succeeded'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "import time\n",
    "while ml_client.online_deployments.get(name = \"blue\", endpoint_name = online_endpoint_name).provisioning_state == 'Updating':\n",
    "    print('Updating..will take about 10 minutes to deploy...')\n",
    "    time.sleep(5)\n",
    "    \n",
    "ml_client.online_deployments.get(name = \"blue\", endpoint_name = online_endpoint_name).provisioning_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Instance status:\\nSystemSetup: Succeeded\\nUserContainerImagePull: Succeeded\\nModelDownload: Succeeded\\nUserContainerStart: Succeeded\\n\\nContainer events:\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:53:47.288403Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:53:52.091811Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:53:57.288271Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:02.091947Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:07.288234Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:12.57254Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:17.288014Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:22.09179Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:27.288003Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:32.091902Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:37.288308Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:42.091922Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:47.288186Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:52.091857Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:54:57.288235Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:55:02.091947Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:55:07.288065Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: LivenessProbeFailed, Type: Warning, Time: 2022-11-30T20:55:12.091788Z, Message: Liveness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ReadinessProbeFailed, Type: Warning, Time: 2022-11-30T20:55:17.288124Z, Message: Readiness probe failed: HTTP probe failed with statuscode: 502\\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2022-11-30T20:55:32.362914545Z, Message: Container is ready\\n\\nContainer logs:\\nSuccessfully installed azure-core-1.26.1 azure-identity-1.12.0 azureml-inference-server-http-0.7.6 cachetools-5.2.0 cffi-1.15.1 cryptography-38.0.4 flask-2.1.3 flask-cors-3.0.10 google-api-core-2.10.2 google-auth-2.14.1 googleapis-common-protos-1.57.0 inference-schema-1.4.2.1 msal-1.20.0 msal-extensions-1.0.0 opencensus-0.11.0 opencensus-context-0.1.3 opencensus-ext-azure-1.1.7 portalocker-2.6.0 psutil-5.9.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 rsa-4.9 typing-extensions-4.4.0 wrapt-1.12.1\\n2022-11-30T20:55:18,144284044+00:00 | gunicorn/run | \\n2022-11-30T20:55:18,146083563+00:00 | gunicorn/run | ###############################################\\n2022-11-30T20:55:18,148017782+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed\\n2022-11-30T20:55:18,149689999+00:00 | gunicorn/run | ###############################################\\n2022-11-30T20:55:18,151357616+00:00 | gunicorn/run | \\n2022-11-30T20:55:18,802266278+00:00 | gunicorn/run | \\n2022-11-30T20:55:18,804003495+00:00 | gunicorn/run | ###############################################\\n2022-11-30T20:55:18,805443510+00:00 | gunicorn/run | AzureML Inference Server\\n2022-11-30T20:55:18,806959125+00:00 | gunicorn/run | ###############################################\\n2022-11-30T20:55:18,808664342+00:00 | gunicorn/run | \\n2022-11-30T20:55:18,810162757+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\\n\\nAzure ML Inferencing HTTP server v0.7.6\\n\\n\\nServer Settings\\n---------------\\nEntry Script Name: /var/mlflow_resources/mlflow_score_script.py\\nModel Directory: /var/azureml-app/azureml-models/chapter6_titanic_model/2\\nWorker Count: 1\\nWorker Timeout (seconds): 300\\nServer Port: 31311\\nApplication Insights Enabled: false\\nApplication Insights Key: None\\nInferencing HTTP server version: azmlinfsrv/0.7.6\\nCORS for the specified origins: None\\n\\n\\nServer Routes\\n---------------\\nLiveness Probe: GET   127.0.0.1:31311/\\nScore:          POST  127.0.0.1:31311/score\\n\\nStarting gunicorn 20.1.0\\nListening at: http://0.0.0.0:31311 (9)\\nUsing worker: sync\\nBooting worker with pid: 695\\nInitializing logger\\n2022-11-30 20:55:19,299 | root | INFO | Starting up app insights client\\nlogging socket not found. logging not available.\\nlogging socket not found. logging not available.\\n2022-11-30 20:55:19,299 | root | INFO | Starting up app insight hooks\\n2022-11-30 20:55:21,161 | root | INFO | Found user script at /var/mlflow_resources/mlflow_score_script.py\\n2022-11-30 20:55:21,161 | root | INFO | run() is decorated with @input_schema. Server will invoke it with the following arguments: input_data.\\n2022-11-30 20:55:21,161 | root | INFO | Invoking user's init function\\n2022-11-30 20:55:21,161 | root | INFO | Users's init has completed successfully\\n2022-11-30 20:55:21,162 | root | INFO | Swaggers are prepared for versions [3] and skipped for versions [2].\\n2022-11-30 20:55:21,162 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\\n2022-11-30 20:55:21,162 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\\n\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get status of online deployment\n",
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f000c1b7eb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking provisioning state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succeeded'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "while ml_client.online_deployments.get(name = \"blue\", endpoint_name = online_endpoint_name).provisioning_state == 'Updating':\n",
    "    print('Updating')\n",
    "    time.sleep(5)\n",
    "    \n",
    "ml_client.online_deployments.get(name = \"blue\", endpoint_name = online_endpoint_name).provisioning_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Endpoint details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Updating', 'scoring_uri': 'https://chp6-mlflow-endpt-11302038545236.eastus.inference.ml.azure.com/score', 'openapi_uri': 'https://chp6-mlflow-endpt-11302038545236.eastus.inference.ml.azure.com/swagger.json', 'name': 'chp6-mlflow-endpt-11302038545236', 'description': 'titanic online endpoint for mlflow model', 'tags': {'mlflow': 'true'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-mlops-dev-rg/providers/microsoft.machinelearningservices/workspaces/mm-aml-mlops-dev/onlineendpoints/chp6-mlflow-endpt-11302038545236', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:01f1a0ff-638e-4ff1-9168-5441f147de81:410c54a4-d056-4da6-bf74-3f4e78dbf443?api-version=2022-02-01-preview'}, 'id': '/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourceGroups/mm-aml-mlops-dev-rg/providers/Microsoft.MachineLearningServices/workspaces/mm-aml-mlops-dev/onlineEndpoints/chp6-mlflow-endpt-11302038545236', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz5/code/Users/memasanz/ML-Engineering-with-Azure-Machine-Learning-Service/chapter 6', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f000fd9af20>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7f0007c3c970>, 'traffic': {'blue': 0}, 'mirror_traffic': {}, 'kind': 'Managed'})\n",
      "{'blue': 0}\n",
      "https://chp6-mlflow-endpt-11302038545236.eastus.inference.ml.azure.com/score\n",
      "2QWTCOc5XlwlwW3NPz7XyLedvqE2GpvO\n"
     ]
    }
   ],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(endpoint)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)\n",
    "\n",
    "primary_key = ml_client.online_endpoints.get_keys(name = online_endpoint_name).primary_key\n",
    "\n",
    "print(primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage the registered ml table from Chapter 4 to get some data to send to the rest endpoint.\n",
    "\n",
    "- Previously in Chapter 4, we registered the MLTable: titanic_prepped_mltable_x2, we will retrieve it and convert it to a pandas datafame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-mlops-dev-rg/workspaces/mm-aml-mlops-dev/datastores/workspaceblobstore/paths/LocalUpload/b3e9d2d76d36b52fc88b17546f0f0460/titanic_prepped_mltable/\n"
     ]
    }
   ],
   "source": [
    "import mltable\n",
    "registered_v1_data_asset = ml_client.data.get(name='titanic_prepped_mltable_x2', version='1')\n",
    "print(registered_v1_data_asset.path)\n",
    "\n",
    "tbl = mltable.load(uri=registered_v1_data_asset.path)\n",
    "df = tbl.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>GroupSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>m</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>S</td>\n",
       "      <td>B</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>f</td>\n",
       "      <td>3</td>\n",
       "      <td>21.5</td>\n",
       "      <td>23.45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Q</td>\n",
       "      <td>X</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Embarked Loc Sex Pclass   Age     Fare GroupSize\n",
       "0          S   X   m      3  22.0     7.25         2\n",
       "1          C   C   f      1  38.0  71.2833         2\n",
       "2          S   X   f      3  26.0    7.925         1\n",
       "3          S   C   f      1  35.0     53.1         2\n",
       "4          S   X   m      3  35.0     8.05         1\n",
       "..       ...  ..  ..    ...   ...      ...       ...\n",
       "886        S   X   m      2  27.0     13.0         1\n",
       "887        S   B   f      1  19.0     30.0         1\n",
       "888        S   X   f      3  21.5    23.45         4\n",
       "889        C   C   m      1  26.0     30.0         1\n",
       "890        Q   X   m      3  32.0     7.75         1\n",
       "\n",
       "[891 rows x 7 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_keep =  ['Embarked', 'Loc', 'Sex','Pclass', 'Age', 'Fare', 'GroupSize']\n",
    "X_raw           = df[columns_to_keep]\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input_data\": {\"columns\": [\"Embarked\", \"Loc\", \"Sex\", \"Pclass\", \"Age\", \"Fare\", \"GroupSize\"], \"data\": [{\"Embarked\": \"S\", \"Loc\": \"X\", \"Sex\": \"m\", \"Pclass\": \"3\", \"Age\": \"22.0\", \"Fare\": \"7.25\", \"GroupSize\": \"2\"}, {\"Embarked\": \"C\", \"Loc\": \"C\", \"Sex\": \"f\", \"Pclass\": \"1\", \"Age\": \"38.0\", \"Fare\": \"71.2833\", \"GroupSize\": \"2\"}]}}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "url = endpoint.scoring_uri\n",
    "api_key = primary_key  # Replace this with the API key for the web service\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
    "import requests\n",
    "\n",
    "def MakePrediction(df):\n",
    "    strjson = str(df.to_json(orient='records'))\n",
    "    endpoint_url = url\n",
    "\n",
    "    request_data =  {\n",
    "              \"input_data\": {\n",
    "                \"columns\": [\n",
    "                  \"Embarked\",\n",
    "                  \"Loc\",\n",
    "                  \"Sex\",\n",
    "                  \"Pclass\",\n",
    "                  \"Age\",\n",
    "                  \"Fare\",\n",
    "                  \"GroupSize\"\n",
    "                ],\n",
    "                \"data\": []\n",
    "              }\n",
    "            }\n",
    "\n",
    "    request_df = X_raw.head(2)\n",
    "    request_data['input_data']['data'] = json.loads(request_df.to_json(orient='records'))\n",
    "    parsed = json.dumps(request_data)\n",
    "    print(parsed)\n",
    "    r = requests.post(endpoint_url, headers=headers, data=parsed)\n",
    "    return (r.json())\n",
    "\n",
    "results = MakePrediction(X_raw.head(2))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "job_env"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
